{
 "cells": [
  {
   "attachments": {
    "Untitled%20Diagram.drawio%20%281%29.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "7b44ee0f",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Semantic textual similarity deals with determining how similar a pair of text documents are. The goal of the first task is to implement a new architecture by combining the ideas from papers\n",
    "- Siamese Recurrent Architectures for Learning Sentence Similarity, Jonas Mueller et. al (will be referred as the AAAI paper)\n",
    "- A Structured Self-Attentive Sentence Embedding, Zhouhan Lin et. al (will be referred as the ICLR paper) <br/><br/>\n",
    "Furthermore, you'd be evaluating whether the new architecture improves the results of **Siamese Recurrent Architectures for Learning Sentence Similarity, Jonas Mueller et. al**. Your overall network architecture should look similar to the following figure. \n",
    "![Untitled%20Diagram.drawio%20%281%29.png](https://raw.githubusercontent.com/shahrukhx01/ocr-test/main/download.png)\n",
    "<br/><br/>\n",
    "\n",
    "\n",
    "Moreover, you'd be required to implement further helper functions that these papers propose i.e., attention penalty term for loss, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbed585c",
   "metadata": {},
   "source": [
    "### SICK dataset\n",
    "We will use SICK dataset throughout the project (at least in the first two tasks). To get more information about the dataset you can refer to the original [paper](http://www.lrec-conf.org/proceedings/lrec2014/pdf/363_Paper.pdf) on the dataset. You can download the dataset using one of the following links:\n",
    "- [dataset page 1](https://marcobaroni.org/composes/sick.html)\n",
    "- [dataset page 2](https://huggingface.co/datasets/sick)    \n",
    "\n",
    "The relevant columns for the project are `sentence_A`, `sentence_B`, `relatedness_score`, where `relatedness_score` is the label. <br><br>\n",
    "**Hint: For each task make sure to decide whether the label should be normalized or not.**<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c132db85",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69b52c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from test import evaluate_test_set\n",
    "import sts_data\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d3a701",
   "metadata": {},
   "source": [
    "## Part 1. Data pipeline (3 points)\n",
    "Before starting working on the model, we must configure the data pipeline to load the data in the correct format. Please, implement the functions for processing the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af7f4ee",
   "metadata": {},
   "source": [
    "### Part 1.1 Loading and preprocessing the data (1 point)\n",
    "Download the SICK dataset and store it in [pandas](https://pandas.pydata.org/docs/index.html) `Dataframe`'s. You should use the official data split.  \n",
    "\n",
    "Implement `load_data` method of `STSData` class in `sts_data.py`. The method must download the dataset and perform basic preprocessing. Minimal preprocessing required:  \n",
    "1. normalize text to lower case\n",
    "2. remove punctuations  \n",
    "3. remove [stopwords](https://en.wikipedia.org/wiki/Stop_word) - we provided you with the list of English stopwords.\n",
    "4. Optionally, any other preprocessing that you deem necessary.\n",
    "\n",
    "All the preprocessing code must be contained in the `preprocessing.py` file.  \n",
    "You can use Hugginface's [datasets library](https://huggingface.co/docs/datasets/) for easy dataset download."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd87f192",
   "metadata": {},
   "source": [
    "### Part 1.2 Building vocabulary (1 point)\n",
    "Before we can feed our text to the model it must be vectorized. We use 300 dimensional pretrained [FastText embeddings](https://fasttext.cc/docs/en/english-vectors.html) for mapping words to vectors. To know more general information about embeddings you can refer to [this video](https://www.youtube.com/watch?v=ERibwqs9p38) (even though, we use different types of embeddings - FastText vs Word2Vec described in the video - the general purpose of them is the same).  \n",
    "In order to apply the embedding, we must first construct the vocabulary for data. Complete the `create_vocab` method of `STSData` class in `sts_data.py` where you concatenate each sentence pair, tokenize it and construct the vocabulary for the whole training data. You should use [torchtext](https://torchtext.readthedocs.io/en/latest/data.html\n",
    ") for processing the data. For tokenization, you can use any library (or write your own tokenizer), but we recommend you to use tokenizer by [spacy](https://spacy.io/). Use the `fasttext.simple.300d` as pretrained vectors.  \n",
    "In the end, you must have a vocabulary object capable of mapping your input to corresponding vectors. Remember that the vocabulary is created using only training data (not touching validation or test data)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b90724d",
   "metadata": {},
   "source": [
    "### Part 1.3 Creating DataLoader (1 point)\n",
    "Implement `get_data_loader` method of `STSData` class in `sts_data.py`. It must perform the following operations on each of the data splits:\n",
    "1. vectorize each pair of the sentences by replacing all tokens with their index in vocabulary\n",
    "2. normalize labels\n",
    "3. convert everything to PyTorch tensors\n",
    "4. pad every sentence so that all of them have the same length\n",
    "5. create `STSDataset` from `dataset.py`\n",
    "6. create PyTorch DataLoader out of the created dataset. \n",
    "\n",
    "\n",
    "We have provided you with the interfaces of possible helper functions, but you can change them as you need.   \n",
    "In the end, you must have 3 data loaders for each of the splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62b40225",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:loading and preprocessing data...\n",
      "WARNING:datasets.builder:Using custom data configuration default\n",
      "WARNING:datasets.builder:Reusing dataset sick (C:\\Users\\laure\\.cache\\huggingface\\datasets\\sick\\default\\0.0.0\\c6b3b0b44eb84b134851396d6d464e5cb8f026960519d640e087fe33472626db)\n",
      "100%|██████████| 3/3 [00:00<00:00, 999.83it/s]\n",
      "INFO:root:Four kids are doing backbends in the park\n",
      "INFO:root:Four children are doing backbends in the park\n",
      "c:\\Users\\laure\\Documents\\NNTI\\nlp_project_nnti 2\\nlp_project_nnti\\task2\\preprocess.py:48: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  dataset['train']['sentence_A'] = dataset['train']['sentence_A'].str.replace(self.punctuation_regex, '')\n",
      "c:\\Users\\laure\\Documents\\NNTI\\nlp_project_nnti 2\\nlp_project_nnti\\task2\\preprocess.py:49: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  dataset['train']['sentence_B'] = dataset['train']['sentence_B'].str.replace(self.punctuation_regex, '')\n",
      "c:\\Users\\laure\\Documents\\NNTI\\nlp_project_nnti 2\\nlp_project_nnti\\task2\\preprocess.py:50: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  dataset['test']['sentence_A'] = dataset['test']['sentence_A'].str.replace(self.punctuation_regex, '')\n",
      "c:\\Users\\laure\\Documents\\NNTI\\nlp_project_nnti 2\\nlp_project_nnti\\task2\\preprocess.py:51: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  dataset['test']['sentence_B'] = dataset['test']['sentence_B'].str.replace(self.punctuation_regex, '')\n",
      "c:\\Users\\laure\\Documents\\NNTI\\nlp_project_nnti 2\\nlp_project_nnti\\task2\\preprocess.py:52: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  dataset['validation']['sentence_A'] = dataset['validation']['sentence_A'].str.replace(self.punctuation_regex, '')\n",
      "c:\\Users\\laure\\Documents\\NNTI\\nlp_project_nnti 2\\nlp_project_nnti\\task2\\preprocess.py:53: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  dataset['validation']['sentence_B'] = dataset['validation']['sentence_B'].str.replace(self.punctuation_regex, '')\n",
      "INFO:root:reading and preprocessing data completed...\n",
      "INFO:root:['kids', 'backbends', 'park']\n",
      "INFO:root:['children', 'backbends', 'park']\n",
      "INFO:root:creating vocabulary...\n",
      "INFO:root:loading spacy tokenizer\n",
      "INFO:root:concatenating sentences\n",
      "INFO:root:building vocab with torchtext\n",
      "INFO:torchtext.vocab.vectors:Loading vectors from .vector_cache\\wiki.simple.vec.pt\n",
      "INFO:root:creating vocabulary completed...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing vocab. Index for kids is 65\n",
      "Testing vocab. Index for sentence ['kids', 'playing', 'yard', 'standing', 'background'] is [65, 4, 144, 6, 141]\n",
      "<unk>\n",
      "<pad>\n",
      "woman\n",
      "Index(['id', 'sentence_A', 'sentence_B', 'label', 'relatedness_score',\n",
      "       'entailment_AB', 'entailment_BA', 'sentence_A_original',\n",
      "       'sentence_B_original', 'sentence_A_dataset', 'sentence_B_dataset'],\n",
      "      dtype='object')\n",
      "max len a: 14, max_len b: 14, max_len 14\n",
      "[228, 93, 7, 572, 368, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "max len a: 12, max_len b: 14, max_len 14\n",
      "[5, 10, 41, 14, 22, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "max len a: 14, max_len b: 14, max_len 14\n",
      "[1347, 12, 16, 164, 1287, 180, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "reload(sts_data)\n",
    "from sts_data import STSData\n",
    "\n",
    "columns_mapping = {\n",
    "        \"sent1\": \"sentence_A\",\n",
    "        \"sent2\": \"sentence_B\",\n",
    "        \"label\": \"relatedness_score\",\n",
    "    }\n",
    "dataset_name = \"sick\"\n",
    "sick_data = STSData(\n",
    "    dataset_name=dataset_name,\n",
    "    columns_mapping=columns_mapping,\n",
    "    normalize_labels=True,\n",
    "    normalization_const=5.0,\n",
    ")\n",
    "batch_size = 128\n",
    "\n",
    "sick_dataloaders = sick_data.get_data_loader(batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98c6526",
   "metadata": {},
   "source": [
    "## Part 2. Model Configuration & Hyperparameter Tuning (3 points)\n",
    "In this part, you are required to define a model capable of learning self-attentive sentence embeddings described in [this ICLR paper](https://arxiv.org/pdf/1703.03130.pdf). The sentence embedding learned by this model will be used for computing the similarity score instead of the simpler embeddings described in the original AAAI paper.  \n",
    "Please familiarize yourself with the model described in the ICLR paper and implement `SiameseBiLSTMAttention` and `SelfAttention` classes in `siamese_lstm_attention.py`. Remember that you must run the model on each sentence in the sentence pair to calculate the similarity between them. You can use `similarity_score` from `utils.py` to compute the similarity score between two sentences. \n",
    "  \n",
    "To get more theoretical information about attention mechanisms you can refer to [this chapter](https://web.stanford.edu/~jurafsky/slp3/10.pdf) of [\"Speech and Language Processing\" book](https://web.stanford.edu/~jurafsky/slp3/) by Dan Jurafsky and James H. Martin, where the attention mechanism is described in the context of the machine translation task. \n",
    "\n",
    "Finally, once your implementation works on the default parameters stated below, make sure to perform **hyperparameter tuning** to find the best combination of hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a530ec28",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_param_search=False\n",
    "\n",
    "output_size = 128\n",
    "hidden_size = 403\n",
    "vocab_size = len(sick_data.vocab)\n",
    "embedding_size = 300\n",
    "embedding_weights = sick_data.vocab.vectors\n",
    "lstm_layers = 4\n",
    "learning_rate = 0.00048586475278877557\n",
    "fc_hidden_size = 89\n",
    "max_epochs = 200\n",
    "bidirectional = True\n",
    "dropout = 0.12354567224086944\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "## self attention config\n",
    "self_attention_config = {\n",
    "    \"hidden_size\": 400,  ## refers to variable 'da' in the ICLR paper\n",
    "    \"output_size\": 14,  ## refers to variable 'r' in the ICLR paper\n",
    "    \"penalty\": 0.1965166853622733,  ## refers to penalty coefficient term in the ICLR paper\n",
    "}\n",
    "\n",
    "attention_encoder_config = {\n",
    "    \"n_layers\": 4,  ## number of encoder layers\n",
    "    \"n_heads\": 16,  ## heads in multi-head attention\n",
    "    \"expansion\": 4, ## encoder feed forward embedding size expansion factor\n",
    "    \"vocab_max\": 15 ## max sequence length\n",
    "}\n",
    "pad_index = sick_data.vocab.stoi['<pad>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89319501",
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import train_model, genetic_hyperparam_search\n",
    "\n",
    "if hyper_param_search:\n",
    "    genetic_hyperparam_search(data_loader=sick_dataloaders,device=device, vocab_size=vocab_size, embedding_weights=embedding_weights,max_epochs=10, num_gens=30, pad_index = pad_index,\n",
    "    config_dict={\n",
    "            \"device\": device,\n",
    "            \"model_name\": \"siamese_lstm_attention\",\n",
    "            \"self_attention_config\": self_attention_config,\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b100de",
   "metadata": {},
   "source": [
    "## Part 3. Training (2 points)  \n",
    "Perform the final training of the model by implementing functions in `train.py` after setting values of your best-chosen hyperparameters. Note you can use the same training function when performing hyperparameter tuning.\n",
    "- **What is a good choice of performance metric here for evaluating your model?** [Max 2-3 lines]\n",
    "- **What other performance evaluation metric can we use here for this task? Motivate your answer.**[Max 2-3 lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad7ee6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Best found hyperparams:hidden_size=80,lstm_layers=2,fc=76,dropout=1e-07,a_hs=98,a_os=21,p=0.7502554332823722, lr=2.225080318923014e-05, e_layers=1\n",
    "## INFO:root:Starting train for model with parameters hidden_size=258,lstm_layers=2,fc=257,dropout=1e-07,a_hs=408,a_os=15,p=0.0, lr=3.967019319929535e-05, e_layers=4\n",
    "##  hidden_size=511,lstm_layers=2,fc=423,dropout=0.0901198887332128,a_hs=428,a_os=19,p=0.0, lr=8e-05, e_layers=4\n",
    "\n",
    "#### After some changes to model (mask)\n",
    "## with parameters hidden_size=529,lstm_layers=4,fc=362,dropout=1e-07,a_hs=128,a_os=18,p=0.0, lr=0.0004930708179433092, e_layers=1, output_size=183\n",
    "\n",
    "batch_size=32\n",
    "output_size = 215\n",
    "hidden_size = 200\n",
    "vocab_size = len(sick_data.vocab)\n",
    "embedding_size = 300\n",
    "embedding_weights = sick_data.vocab.vectors\n",
    "lstm_layers = 5\n",
    "learning_rate = 8e-05#0.0004930708179433092\n",
    "fc_hidden_size = 256\n",
    "max_epochs = 200\n",
    "bidirectional = True\n",
    "dropout = 0.166\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "## self attention config\n",
    "self_attention_config = {\n",
    "    \"hidden_size\": 400,  ## refers to variable 'da' in the ICLR paper\n",
    "    \"output_size\": 20,  ## refers to variable 'r' in the ICLR paper\n",
    "    \"penalty\": 0.0,  ## refers to penalty coefficient term in the ICLR paper\n",
    "}\n",
    "\n",
    "attention_encoder_config = {\n",
    "    \"n_layers\": 12,  ## number of encoder layers\n",
    "    \"n_heads\": 6,  ## heads in multi-head attention\n",
    "    \"expansion\": 4, ## encoder feed forward embedding size expansion factor\n",
    "    \"vocab_max\": 20 ## max sequence length\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c06eb69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SiameseBiLSTMAttention(\n",
       "  (word_embeddings): Embedding(1962, 300)\n",
       "  (encoder): Encoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): EncoderBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (Q): Linear(in_features=300, out_features=300, bias=False)\n",
       "          (K): Linear(in_features=300, out_features=300, bias=False)\n",
       "          (V): Linear(in_features=300, out_features=300, bias=False)\n",
       "          (output_layer): Linear(in_features=300, out_features=300, bias=True)\n",
       "        )\n",
       "        (Dropout): Dropout(p=0.166, inplace=False)\n",
       "        (LayerNorm1): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "        (LayerNorm2): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "        (FeedForward): Sequential(\n",
       "          (0): Linear(in_features=300, out_features=1200, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1200, out_features=300, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (1): EncoderBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (Q): Linear(in_features=300, out_features=300, bias=False)\n",
       "          (K): Linear(in_features=300, out_features=300, bias=False)\n",
       "          (V): Linear(in_features=300, out_features=300, bias=False)\n",
       "          (output_layer): Linear(in_features=300, out_features=300, bias=True)\n",
       "        )\n",
       "        (Dropout): Dropout(p=0.166, inplace=False)\n",
       "        (LayerNorm1): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "        (LayerNorm2): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "        (FeedForward): Sequential(\n",
       "          (0): Linear(in_features=300, out_features=1200, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1200, out_features=300, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (2): EncoderBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (Q): Linear(in_features=300, out_features=300, bias=False)\n",
       "          (K): Linear(in_features=300, out_features=300, bias=False)\n",
       "          (V): Linear(in_features=300, out_features=300, bias=False)\n",
       "          (output_layer): Linear(in_features=300, out_features=300, bias=True)\n",
       "        )\n",
       "        (Dropout): Dropout(p=0.166, inplace=False)\n",
       "        (LayerNorm1): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "        (LayerNorm2): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "        (FeedForward): Sequential(\n",
       "          (0): Linear(in_features=300, out_features=1200, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1200, out_features=300, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (3): EncoderBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (Q): Linear(in_features=300, out_features=300, bias=False)\n",
       "          (K): Linear(in_features=300, out_features=300, bias=False)\n",
       "          (V): Linear(in_features=300, out_features=300, bias=False)\n",
       "          (output_layer): Linear(in_features=300, out_features=300, bias=True)\n",
       "        )\n",
       "        (Dropout): Dropout(p=0.166, inplace=False)\n",
       "        (LayerNorm1): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "        (LayerNorm2): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "        (FeedForward): Sequential(\n",
       "          (0): Linear(in_features=300, out_features=1200, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1200, out_features=300, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (4): EncoderBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (Q): Linear(in_features=300, out_features=300, bias=False)\n",
       "          (K): Linear(in_features=300, out_features=300, bias=False)\n",
       "          (V): Linear(in_features=300, out_features=300, bias=False)\n",
       "          (output_layer): Linear(in_features=300, out_features=300, bias=True)\n",
       "        )\n",
       "        (Dropout): Dropout(p=0.166, inplace=False)\n",
       "        (LayerNorm1): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "        (LayerNorm2): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "        (FeedForward): Sequential(\n",
       "          (0): Linear(in_features=300, out_features=1200, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1200, out_features=300, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (5): EncoderBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (Q): Linear(in_features=300, out_features=300, bias=False)\n",
       "          (K): Linear(in_features=300, out_features=300, bias=False)\n",
       "          (V): Linear(in_features=300, out_features=300, bias=False)\n",
       "          (output_layer): Linear(in_features=300, out_features=300, bias=True)\n",
       "        )\n",
       "        (Dropout): Dropout(p=0.166, inplace=False)\n",
       "        (LayerNorm1): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "        (LayerNorm2): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "        (FeedForward): Sequential(\n",
       "          (0): Linear(in_features=300, out_features=1200, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1200, out_features=300, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (6): EncoderBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (Q): Linear(in_features=300, out_features=300, bias=False)\n",
       "          (K): Linear(in_features=300, out_features=300, bias=False)\n",
       "          (V): Linear(in_features=300, out_features=300, bias=False)\n",
       "          (output_layer): Linear(in_features=300, out_features=300, bias=True)\n",
       "        )\n",
       "        (Dropout): Dropout(p=0.166, inplace=False)\n",
       "        (LayerNorm1): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "        (LayerNorm2): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "        (FeedForward): Sequential(\n",
       "          (0): Linear(in_features=300, out_features=1200, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1200, out_features=300, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (7): EncoderBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (Q): Linear(in_features=300, out_features=300, bias=False)\n",
       "          (K): Linear(in_features=300, out_features=300, bias=False)\n",
       "          (V): Linear(in_features=300, out_features=300, bias=False)\n",
       "          (output_layer): Linear(in_features=300, out_features=300, bias=True)\n",
       "        )\n",
       "        (Dropout): Dropout(p=0.166, inplace=False)\n",
       "        (LayerNorm1): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "        (LayerNorm2): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "        (FeedForward): Sequential(\n",
       "          (0): Linear(in_features=300, out_features=1200, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1200, out_features=300, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (8): EncoderBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (Q): Linear(in_features=300, out_features=300, bias=False)\n",
       "          (K): Linear(in_features=300, out_features=300, bias=False)\n",
       "          (V): Linear(in_features=300, out_features=300, bias=False)\n",
       "          (output_layer): Linear(in_features=300, out_features=300, bias=True)\n",
       "        )\n",
       "        (Dropout): Dropout(p=0.166, inplace=False)\n",
       "        (LayerNorm1): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "        (LayerNorm2): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "        (FeedForward): Sequential(\n",
       "          (0): Linear(in_features=300, out_features=1200, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1200, out_features=300, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (9): EncoderBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (Q): Linear(in_features=300, out_features=300, bias=False)\n",
       "          (K): Linear(in_features=300, out_features=300, bias=False)\n",
       "          (V): Linear(in_features=300, out_features=300, bias=False)\n",
       "          (output_layer): Linear(in_features=300, out_features=300, bias=True)\n",
       "        )\n",
       "        (Dropout): Dropout(p=0.166, inplace=False)\n",
       "        (LayerNorm1): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "        (LayerNorm2): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "        (FeedForward): Sequential(\n",
       "          (0): Linear(in_features=300, out_features=1200, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1200, out_features=300, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (10): EncoderBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (Q): Linear(in_features=300, out_features=300, bias=False)\n",
       "          (K): Linear(in_features=300, out_features=300, bias=False)\n",
       "          (V): Linear(in_features=300, out_features=300, bias=False)\n",
       "          (output_layer): Linear(in_features=300, out_features=300, bias=True)\n",
       "        )\n",
       "        (Dropout): Dropout(p=0.166, inplace=False)\n",
       "        (LayerNorm1): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "        (LayerNorm2): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "        (FeedForward): Sequential(\n",
       "          (0): Linear(in_features=300, out_features=1200, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1200, out_features=300, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (11): EncoderBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (Q): Linear(in_features=300, out_features=300, bias=False)\n",
       "          (K): Linear(in_features=300, out_features=300, bias=False)\n",
       "          (V): Linear(in_features=300, out_features=300, bias=False)\n",
       "          (output_layer): Linear(in_features=300, out_features=300, bias=True)\n",
       "        )\n",
       "        (Dropout): Dropout(p=0.166, inplace=False)\n",
       "        (LayerNorm1): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "        (LayerNorm2): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "        (FeedForward): Sequential(\n",
       "          (0): Linear(in_features=300, out_features=1200, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1200, out_features=300, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pos_embedding): Embedding(20, 300)\n",
       "    (Dropout): Dropout(p=0.166, inplace=False)\n",
       "  )\n",
       "  (biLSTM): LSTM(300, 200, num_layers=5, dropout=0.166, bidirectional=True)\n",
       "  (attention): SelfAttention(\n",
       "    (Ws1): Linear(in_features=400, out_features=400, bias=False)\n",
       "    (Ws2): Linear(in_features=400, out_features=20, bias=False)\n",
       "  )\n",
       "  (fc_layer): Sequential(\n",
       "    (0): Linear(in_features=8000, out_features=256, bias=False)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=256, out_features=215, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from siamese_lstm_attention import SiameseBiLSTMAttention\n",
    "## init siamese lstm\n",
    "siamese_lstm_attention = SiameseBiLSTMAttention(\n",
    "    batch_size=batch_size,\n",
    "    output_size=output_size,\n",
    "    hidden_size=hidden_size,\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_size=embedding_size,\n",
    "    embedding_weights=embedding_weights,\n",
    "    lstm_layers=lstm_layers,\n",
    "    self_attention_config=self_attention_config,\n",
    "    attention_encoder_config=attention_encoder_config,\n",
    "    fc_hidden_size=fc_hidden_size,\n",
    "    device=device,\n",
    "    bidirectional=bidirectional,\n",
    "    pad_index=pad_index,\n",
    "    dropout=dropout\n",
    ")\n",
    "## move model to device\n",
    "siamese_lstm_attention.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "354d7c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]INFO:root:Epoch: 0\n",
      "INFO:root:new model saved\n",
      "INFO:root:Train loss: 0.02753680758178234 - acc: 0.3448813992969405 -- Validation loss: 0.007506425492465496 - acc: 0.26822569939926144\n",
      "  0%|          | 1/200 [00:03<12:12,  3.68s/it]INFO:root:Epoch: 1\n",
      "INFO:root:new model saved\n",
      "INFO:root:Train loss: 0.021194856613874435 - acc: 0.496366920177309 -- Validation loss: 0.008252247236669064 - acc: 0.31223902662586156\n",
      "  1%|          | 2/200 [00:07<11:42,  3.55s/it]INFO:root:Epoch: 2\n",
      "INFO:root:Train loss: 0.017865486443042755 - acc: 0.5757825004713015 -- Validation loss: 0.006526308134198189 - acc: 0.247161787280618\n",
      "  2%|▏         | 3/200 [00:10<11:19,  3.45s/it]INFO:root:Epoch: 3\n",
      "INFO:root:new model saved\n",
      "INFO:root:Train loss: 0.016477469354867935 - acc: 0.6094650852876989 -- Validation loss: 0.005379900336265564 - acc: 0.49895183688830924\n",
      "  2%|▏         | 4/200 [00:13<11:15,  3.44s/it]INFO:root:Epoch: 4\n",
      "INFO:root:new model saved\n",
      "INFO:root:Train loss: 0.015269936993718147 - acc: 0.6372409263970875 -- Validation loss: 0.0052602156065404415 - acc: 0.5013300291209674\n",
      "  2%|▎         | 5/200 [00:17<11:17,  3.47s/it]INFO:root:Epoch: 5\n",
      "INFO:root:Train loss: 0.013765999116003513 - acc: 0.673126476668849 -- Validation loss: 0.0066819400526583195 - acc: 0.4255324959567134\n",
      "  3%|▎         | 6/200 [00:21<11:29,  3.55s/it]INFO:root:Epoch: 6\n",
      "INFO:root:Train loss: 0.0129194101318717 - acc: 0.6932225074605822 -- Validation loss: 0.006436455063521862 - acc: 0.20768908849538614\n",
      "  4%|▎         | 7/200 [00:24<11:32,  3.59s/it]INFO:root:Epoch: 7\n",
      "INFO:root:Train loss: 0.012813822366297245 - acc: 0.6953573901818334 -- Validation loss: 0.006291661411523819 - acc: 0.35961142000422786\n",
      "  4%|▍         | 8/200 [00:28<11:27,  3.58s/it]INFO:root:Epoch: 8\n",
      "INFO:root:Train loss: 0.01225036196410656 - acc: 0.7097833014421244 -- Validation loss: 0.007915743626654148 - acc: 0.33094926245898093\n",
      "  4%|▍         | 9/200 [00:31<11:19,  3.56s/it]INFO:root:Epoch: 9\n",
      "INFO:root:new model saved\n",
      "INFO:root:Train loss: 0.011732726357877254 - acc: 0.7213107184769727 -- Validation loss: 0.005947265774011612 - acc: 0.5316689872346483\n",
      "  5%|▌         | 10/200 [00:35<11:22,  3.59s/it]INFO:root:Epoch: 10\n",
      "INFO:root:Train loss: 0.011198271997272968 - acc: 0.733966835899297 -- Validation loss: 0.006274818442761898 - acc: 0.47298537718175604\n",
      "  6%|▌         | 11/200 [00:39<11:17,  3.58s/it]INFO:root:Epoch: 11\n",
      "INFO:root:Train loss: 0.01102637778967619 - acc: 0.7382872743342905 -- Validation loss: 0.006057392340153456 - acc: 0.44263891239108466\n",
      "  6%|▌         | 12/200 [00:42<11:10,  3.57s/it]INFO:root:Epoch: 12\n",
      "INFO:root:Train loss: 0.010797044262290001 - acc: 0.7433307609170816 -- Validation loss: 0.005265912972390652 - acc: 0.5157624513733922\n",
      "  6%|▋         | 13/200 [00:46<11:05,  3.56s/it]INFO:root:Epoch: 13\n",
      "INFO:root:Train loss: 0.010535337962210178 - acc: 0.7499007714692867 -- Validation loss: 0.005798996891826391 - acc: 0.4807763737571855\n",
      "  7%|▋         | 14/200 [00:49<11:03,  3.56s/it]INFO:root:Epoch: 14\n",
      "INFO:root:Train loss: 0.01062084175646305 - acc: 0.747919952939977 -- Validation loss: 0.005415318999439478 - acc: 0.37631718392357816\n",
      "  8%|▊         | 15/200 [00:53<10:57,  3.56s/it]INFO:root:Epoch: 15\n",
      "INFO:root:Train loss: 0.01034136489033699 - acc: 0.7548646271982303 -- Validation loss: 0.005815388169139624 - acc: 0.31005049511671323\n",
      "  8%|▊         | 16/200 [00:56<10:53,  3.55s/it]INFO:root:Epoch: 16\n",
      "INFO:root:Train loss: 0.010002736933529377 - acc: 0.7622958810540563 -- Validation loss: 0.005135131534188986 - acc: 0.34930887536015165\n",
      "  8%|▊         | 17/200 [01:00<10:51,  3.56s/it]INFO:root:Epoch: 17\n",
      "INFO:root:Train loss: 0.009733765386044979 - acc: 0.7692234910062048 -- Validation loss: 0.006397073622792959 - acc: 0.31129820434443944\n",
      "  9%|▉         | 18/200 [01:03<10:49,  3.57s/it]INFO:root:Epoch: 18\n",
      "INFO:root:Train loss: 0.009488578885793686 - acc: 0.7749098659769034 -- Validation loss: 0.006363833323121071 - acc: 0.37562797121342395\n",
      " 10%|▉         | 19/200 [01:07<10:44,  3.56s/it]INFO:root:Epoch: 19\n",
      "INFO:root:new model saved\n",
      "INFO:root:Train loss: 0.009435568004846573 - acc: 0.7760489997520975 -- Validation loss: 0.005762995686382055 - acc: 0.5337279976690028\n",
      " 10%|█         | 20/200 [01:11<10:43,  3.58s/it]INFO:root:Epoch: 20\n",
      "INFO:root:Train loss: 0.009473485872149467 - acc: 0.7757277479896618 -- Validation loss: 0.007601312827318907 - acc: 0.28794804142711716\n",
      " 10%|█         | 21/200 [01:14<10:39,  3.57s/it]INFO:root:Epoch: 21\n",
      "INFO:root:new model saved\n",
      "INFO:root:Train loss: 0.009240595623850822 - acc: 0.7812121713007589 -- Validation loss: 0.004615689627826214 - acc: 0.5737377713045859\n",
      " 11%|█         | 22/200 [01:18<10:42,  3.61s/it]INFO:root:Epoch: 22\n",
      "INFO:root:Train loss: 0.009225034154951572 - acc: 0.7813553567860858 -- Validation loss: 0.006350947078317404 - acc: 0.22485299344556742\n",
      " 12%|█▏        | 23/200 [01:21<10:36,  3.60s/it]INFO:root:Epoch: 23\n",
      "INFO:root:Train loss: 0.009030651301145554 - acc: 0.7856556697232641 -- Validation loss: 0.005456342361867428 - acc: 0.46343715316307754\n",
      " 12%|█▏        | 24/200 [01:25<10:34,  3.60s/it]INFO:root:Epoch: 24\n",
      "INFO:root:new model saved\n",
      "INFO:root:Train loss: 0.008966011926531792 - acc: 0.7877141915330524 -- Validation loss: 0.00531330332159996 - acc: 0.6046973059182008\n",
      " 12%|█▎        | 25/200 [01:29<10:36,  3.64s/it]INFO:root:Epoch: 25\n",
      "INFO:root:Train loss: 0.00892420019954443 - acc: 0.7886065492868206 -- Validation loss: 0.006643272936344147 - acc: 0.21706034688120335\n",
      " 13%|█▎        | 26/200 [01:32<10:27,  3.61s/it]INFO:root:Epoch: 26\n",
      "INFO:root:Train loss: 0.008753820322453976 - acc: 0.7923369977986402 -- Validation loss: 0.00518395658582449 - acc: 0.49154456957735604\n",
      " 14%|█▎        | 27/200 [01:36<10:23,  3.60s/it]INFO:root:Epoch: 27\n",
      "INFO:root:Train loss: 0.008706126362085342 - acc: 0.7933079079251482 -- Validation loss: 0.00682109035551548 - acc: 0.3328815847591423\n",
      " 14%|█▍        | 28/200 [01:40<10:22,  3.62s/it]INFO:root:Epoch: 28\n",
      "INFO:root:Train loss: 0.008416670374572277 - acc: 0.7997624711550353 -- Validation loss: 0.005838777869939804 - acc: 0.33740944237595005\n",
      " 14%|█▍        | 29/200 [01:43<10:32,  3.70s/it]INFO:root:Epoch: 29\n",
      "INFO:root:Train loss: 0.008574528619647026 - acc: 0.7964038116841569 -- Validation loss: 0.007085087243467569 - acc: 0.2944674783689911\n",
      " 15%|█▌        | 30/200 [01:47<10:27,  3.69s/it]INFO:root:Epoch: 30\n",
      "INFO:root:Train loss: 0.008576578460633755 - acc: 0.7963232273272223 -- Validation loss: 0.006219467148184776 - acc: 0.35215591772871957\n",
      " 16%|█▌        | 31/200 [01:51<10:19,  3.67s/it]INFO:root:Epoch: 31\n",
      "INFO:root:Train loss: 0.008330312557518482 - acc: 0.8025862368997353 -- Validation loss: 0.0043319351971149445 - acc: 0.5395008241103609\n",
      " 16%|█▌        | 32/200 [01:54<10:13,  3.65s/it]INFO:root:Epoch: 32\n",
      "INFO:root:Train loss: 0.008207778446376324 - acc: 0.8047365766288908 -- Validation loss: 0.0058234683237969875 - acc: 0.3449600491305158\n",
      " 16%|█▋        | 33/200 [01:58<10:05,  3.63s/it]INFO:root:Epoch: 33\n",
      "INFO:root:Train loss: 0.008226604200899601 - acc: 0.8048971622408087 -- Validation loss: 0.006013806909322739 - acc: 0.5342739452125667\n",
      " 17%|█▋        | 34/200 [02:02<10:02,  3.63s/it]INFO:root:Epoch: 34\n",
      "INFO:root:Train loss: 0.00835817027837038 - acc: 0.8021408665236236 -- Validation loss: 0.006169391795992851 - acc: 0.2824717669246609\n",
      " 18%|█▊        | 35/200 [02:05<09:58,  3.63s/it]INFO:root:Epoch: 35\n",
      "INFO:root:Train loss: 0.008272753097116947 - acc: 0.8035170788561303 -- Validation loss: 0.006175102666020393 - acc: 0.4764928190885134\n",
      " 18%|█▊        | 36/200 [02:09<09:56,  3.63s/it]INFO:root:Epoch: 36\n",
      "INFO:root:Train loss: 0.008302322588860989 - acc: 0.8028103640208638 -- Validation loss: 0.005886507220566273 - acc: 0.3479854768416286\n",
      " 18%|█▊        | 37/200 [02:12<09:51,  3.63s/it]INFO:root:Epoch: 37\n",
      "INFO:root:Train loss: 0.008133172988891602 - acc: 0.8070883780083244 -- Validation loss: 0.0066588218323886395 - acc: 0.36404946233347435\n",
      " 19%|█▉        | 38/200 [02:16<09:47,  3.63s/it]INFO:root:Epoch: 38\n",
      "INFO:root:Train loss: 0.008117269724607468 - acc: 0.8076846109508984 -- Validation loss: 0.005903406534343958 - acc: 0.30444715976189574\n",
      " 20%|█▉        | 39/200 [02:20<09:43,  3.63s/it]INFO:root:Epoch: 39\n",
      "INFO:root:Train loss: 0.007999147288501263 - acc: 0.8099052839261318 -- Validation loss: 0.006164381746202707 - acc: 0.4688912555516146\n",
      " 20%|██        | 40/200 [02:23<09:35,  3.60s/it]INFO:root:Epoch: 40\n",
      "INFO:root:Train loss: 0.008091666735708714 - acc: 0.8076501306326186 -- Validation loss: 0.008584495633840561 - acc: -0.04604696948150866\n",
      " 20%|██        | 41/200 [02:27<09:31,  3.60s/it]INFO:root:Epoch: 41\n",
      "INFO:root:Train loss: 0.007840721867978573 - acc: 0.8139638092682963 -- Validation loss: 0.0061133005656301975 - acc: 0.3190879889382271\n",
      " 21%|██        | 42/200 [02:30<09:28,  3.60s/it]INFO:root:Epoch: 42\n",
      "INFO:root:Train loss: 0.007742919493466616 - acc: 0.8164440969279123 -- Validation loss: 0.006369481794536114 - acc: 0.4292275655957999\n",
      " 22%|██▏       | 43/200 [02:34<09:27,  3.61s/it]INFO:root:Epoch: 43\n",
      "INFO:root:Train loss: 0.008016543462872505 - acc: 0.8093567963978375 -- Validation loss: 0.005324597470462322 - acc: 0.594682532469074\n",
      " 22%|██▏       | 44/200 [02:38<09:23,  3.61s/it]INFO:root:Epoch: 44\n",
      "INFO:root:Train loss: 0.008029935881495476 - acc: 0.8098237578286182 -- Validation loss: 0.00504860607907176 - acc: 0.5121295946937661\n",
      " 22%|██▎       | 45/200 [02:41<09:20,  3.61s/it]INFO:root:Epoch: 45\n",
      "INFO:root:Train loss: 0.008024380542337894 - acc: 0.8101330437269606 -- Validation loss: 0.0060928030870854855 - acc: 0.2759322338799297\n",
      " 23%|██▎       | 46/200 [02:45<09:18,  3.63s/it]INFO:root:Epoch: 46\n",
      "INFO:root:Train loss: 0.007642228156328201 - acc: 0.8185066785133782 -- Validation loss: 0.005818008910864592 - acc: 0.4566059639768125\n",
      " 24%|██▎       | 47/200 [02:49<09:14,  3.63s/it]INFO:root:Epoch: 47\n",
      "INFO:root:Train loss: 0.0077622439712285995 - acc: 0.8160177444161906 -- Validation loss: 0.004648635163903236 - acc: 0.5700584912182037\n",
      " 24%|██▍       | 48/200 [02:52<09:08,  3.61s/it]INFO:root:Epoch: 48\n",
      "INFO:root:Train loss: 0.007593302056193352 - acc: 0.8199421515574493 -- Validation loss: 0.005002070218324661 - acc: 0.5161073523128562\n",
      " 24%|██▍       | 49/200 [02:56<09:05,  3.61s/it]INFO:root:Epoch: 49\n",
      "INFO:root:Train loss: 0.00765641313046217 - acc: 0.8181987447131549 -- Validation loss: 0.005503307096660137 - acc: 0.48912922107391354\n",
      " 25%|██▌       | 50/200 [02:59<09:01,  3.61s/it]INFO:root:Epoch: 50\n",
      "INFO:root:Train loss: 0.007778388448059559 - acc: 0.8149484717055514 -- Validation loss: 0.007100286893546581 - acc: 0.20495324321160013\n",
      " 26%|██▌       | 51/200 [03:03<08:56,  3.60s/it]INFO:root:Epoch: 51\n",
      "INFO:root:Train loss: 0.00767173245549202 - acc: 0.8179133462454462 -- Validation loss: 0.005101684480905533 - acc: 0.4874059586432522\n",
      " 26%|██▌       | 52/200 [03:07<08:52,  3.60s/it]INFO:root:Epoch: 52\n",
      "INFO:root:Train loss: 0.007749774493277073 - acc: 0.8161386120740262 -- Validation loss: 0.005526325665414333 - acc: 0.2722148116140367\n",
      " 26%|██▋       | 53/200 [03:10<08:50,  3.61s/it]INFO:root:Epoch: 53\n",
      "INFO:root:Train loss: 0.007684642914682627 - acc: 0.817863805056106 -- Validation loss: 0.004664176143705845 - acc: 0.5886966917376899\n",
      " 27%|██▋       | 54/200 [03:14<08:49,  3.63s/it]INFO:root:Epoch: 54\n",
      "INFO:root:Train loss: 0.00754851009696722 - acc: 0.8209991216526731 -- Validation loss: 0.006990197580307722 - acc: 0.2993230612536697\n",
      " 28%|██▊       | 55/200 [03:18<08:54,  3.69s/it]INFO:root:Epoch: 55\n",
      "INFO:root:Train loss: 0.007659040857106447 - acc: 0.8182846864124429 -- Validation loss: 0.005278964526951313 - acc: 0.49728270441628974\n",
      " 28%|██▊       | 56/200 [03:21<08:48,  3.67s/it]INFO:root:Epoch: 56\n",
      "INFO:root:Train loss: 0.007503166329115629 - acc: 0.8223908671225392 -- Validation loss: 0.006228288169950247 - acc: 0.4356302942902798\n",
      " 28%|██▊       | 57/200 [03:25<08:50,  3.71s/it]INFO:root:Epoch: 57\n",
      "INFO:root:Train loss: 0.007484086323529482 - acc: 0.8228621964984316 -- Validation loss: 0.006112108007073402 - acc: 0.42026828065776056\n",
      " 29%|██▉       | 58/200 [03:29<08:54,  3.76s/it]INFO:root:Epoch: 58\n",
      "INFO:root:Train loss: 0.007581151556223631 - acc: 0.8201572337854157 -- Validation loss: 0.005594607908278704 - acc: 0.5522054076295138\n",
      " 30%|██▉       | 59/200 [03:33<08:52,  3.78s/it]INFO:root:Epoch: 59\n",
      "INFO:root:Train loss: 0.007576235570013523 - acc: 0.8202383748422817 -- Validation loss: 0.0068587008863687515 - acc: 0.3074537967306942\n",
      " 30%|███       | 60/200 [03:37<08:49,  3.78s/it]INFO:root:Epoch: 60\n",
      "INFO:root:Train loss: 0.007395430002361536 - acc: 0.824135282390959 -- Validation loss: 0.005615550093352795 - acc: 0.47142846609828326\n",
      " 30%|███       | 61/200 [03:40<08:37,  3.72s/it]INFO:root:Epoch: 61\n",
      "INFO:root:Train loss: 0.007410918828099966 - acc: 0.8246801003199654 -- Validation loss: 0.005453379824757576 - acc: 0.4498746691543153\n",
      " 31%|███       | 62/200 [03:44<08:29,  3.69s/it]INFO:root:Epoch: 62\n",
      "INFO:root:Train loss: 0.007530213333666325 - acc: 0.8216562480007276 -- Validation loss: 0.006881905719637871 - acc: 0.230354008941691\n",
      " 32%|███▏      | 63/200 [03:47<08:24,  3.69s/it]INFO:root:Epoch: 63\n",
      "INFO:root:Train loss: 0.007494875229895115 - acc: 0.8221906801733856 -- Validation loss: 0.006002052221447229 - acc: 0.4232685593423433\n",
      " 32%|███▏      | 64/200 [03:51<08:19,  3.67s/it]INFO:root:Epoch: 64\n",
      "INFO:root:Train loss: 0.007390307728201151 - acc: 0.8248574548561296 -- Validation loss: 0.007915966212749481 - acc: 0.08303141585791374\n",
      " 32%|███▎      | 65/200 [03:55<08:11,  3.64s/it]INFO:root:Epoch: 65\n",
      "INFO:root:Train loss: 0.0073840199038386345 - acc: 0.825241246549383 -- Validation loss: 0.0061152041889727116 - acc: 0.2840315427484498\n",
      " 33%|███▎      | 66/200 [03:58<08:06,  3.63s/it]INFO:root:Epoch: 66\n",
      "INFO:root:Train loss: 0.007533512078225613 - acc: 0.8210706202880128 -- Validation loss: 0.005609410349279642 - acc: 0.45849447157088097\n",
      " 34%|███▎      | 67/200 [04:02<08:03,  3.64s/it]INFO:root:Epoch: 67\n",
      "INFO:root:Train loss: 0.007380729075521231 - acc: 0.8249748824217774 -- Validation loss: 0.0065467883832752705 - acc: 0.295140544220421\n",
      " 34%|███▍      | 68/200 [04:06<08:00,  3.64s/it]INFO:root:Epoch: 68\n",
      "INFO:root:Train loss: 0.007381882052868605 - acc: 0.8245765797647282 -- Validation loss: 0.005824890453368425 - acc: 0.4068738071519422\n",
      " 34%|███▍      | 69/200 [04:09<07:55,  3.63s/it]INFO:root:Epoch: 69\n",
      "INFO:root:Train loss: 0.007331433705985546 - acc: 0.8258153470112068 -- Validation loss: 0.004824938252568245 - acc: 0.5705468359351322\n",
      " 35%|███▌      | 70/200 [04:13<07:53,  3.65s/it]INFO:root:Epoch: 70\n",
      "INFO:root:Train loss: 0.007399154826998711 - acc: 0.8241905686178195 -- Validation loss: 0.00531768798828125 - acc: 0.4481366407232332\n",
      " 36%|███▌      | 71/200 [04:17<07:51,  3.66s/it]INFO:root:Epoch: 71\n",
      "INFO:root:Train loss: 0.007246226072311401 - acc: 0.8290042434971185 -- Validation loss: 0.006638175807893276 - acc: 0.24917958988424915\n",
      " 36%|███▌      | 72/200 [04:20<07:46,  3.65s/it]INFO:root:Epoch: 72\n",
      "INFO:root:Train loss: 0.007351728156208992 - acc: 0.8256963216957762 -- Validation loss: 0.005718748550862074 - acc: 0.41714540453427573\n",
      " 36%|███▋      | 73/200 [04:24<07:43,  3.65s/it]INFO:root:Epoch: 73\n",
      "INFO:root:Train loss: 0.007325360551476479 - acc: 0.8262982101535253 -- Validation loss: 0.0061858766712248325 - acc: 0.3257910033281982\n",
      " 37%|███▋      | 74/200 [04:28<07:40,  3.65s/it]INFO:root:Epoch: 74\n",
      "INFO:root:Train loss: 0.007272509858012199 - acc: 0.8270846061242532 -- Validation loss: 0.0055436743423342705 - acc: 0.44861303326075563\n",
      " 38%|███▊      | 75/200 [04:31<07:35,  3.65s/it]INFO:root:Epoch: 75\n",
      "INFO:root:Train loss: 0.007264068350195885 - acc: 0.8278290429920807 -- Validation loss: 0.008176589384675026 - acc: 0.013492454010307076\n",
      " 38%|███▊      | 76/200 [04:35<07:32,  3.65s/it]INFO:root:Epoch: 76\n",
      "INFO:root:Train loss: 0.007121671922504902 - acc: 0.8308532688476387 -- Validation loss: 0.0060792043805122375 - acc: 0.3835738658917883\n",
      " 38%|███▊      | 77/200 [04:39<07:30,  3.67s/it]INFO:root:Epoch: 77\n",
      "INFO:root:Train loss: 0.007066576741635799 - acc: 0.8322341758763808 -- Validation loss: 0.005705492105334997 - acc: 0.4737453705509901\n",
      " 39%|███▉      | 78/200 [04:42<07:25,  3.65s/it]INFO:root:Epoch: 78\n",
      "INFO:root:Train loss: 0.007193028926849365 - acc: 0.8292343304604637 -- Validation loss: 0.007623727433383465 - acc: 0.16680607240591128\n",
      " 40%|███▉      | 79/200 [04:46<07:19,  3.64s/it]INFO:root:Epoch: 79\n",
      "INFO:root:Train loss: 0.0070435102097690105 - acc: 0.8325678201357958 -- Validation loss: 0.0061743673868477345 - acc: 0.1942913855307954\n",
      " 40%|████      | 80/200 [04:49<07:15,  3.63s/it]INFO:root:Epoch: 80\n",
      "INFO:root:Train loss: 0.007061542943120003 - acc: 0.8326386558723489 -- Validation loss: 0.007344994693994522 - acc: 0.41347917685649094\n",
      " 40%|████      | 81/200 [04:53<07:14,  3.65s/it]INFO:root:Epoch: 81\n",
      "INFO:root:Train loss: 0.006892351433634758 - acc: 0.836135876021668 -- Validation loss: 0.005566249135881662 - acc: 0.47229626215369125\n",
      " 41%|████      | 82/200 [04:57<07:09,  3.64s/it]INFO:root:Epoch: 82\n",
      "INFO:root:Train loss: 0.007048475090414286 - acc: 0.8327816882946417 -- Validation loss: 0.008009817451238632 - acc: 0.17679926257944023\n",
      " 42%|████▏     | 83/200 [05:00<07:08,  3.66s/it]INFO:root:Epoch: 83\n",
      "INFO:root:Train loss: 0.006983366794884205 - acc: 0.8345700242042913 -- Validation loss: 0.006663928739726543 - acc: 0.32568882804397037\n",
      " 42%|████▏     | 84/200 [05:04<07:04,  3.66s/it]INFO:root:Epoch: 84\n",
      "INFO:root:new model saved\n",
      "INFO:root:Train loss: 0.007096762768924236 - acc: 0.8319396918949773 -- Validation loss: 0.004108301363885403 - acc: 0.6186892867389642\n",
      " 42%|████▎     | 85/200 [05:08<07:04,  3.69s/it]INFO:root:Epoch: 85\n",
      "INFO:root:Train loss: 0.006970296613872051 - acc: 0.8341958320778066 -- Validation loss: 0.00629222858697176 - acc: 0.2694156450876606\n",
      " 43%|████▎     | 86/200 [05:11<06:57,  3.67s/it]INFO:root:Epoch: 86\n",
      "INFO:root:Train loss: 0.00692892586812377 - acc: 0.8361152260598852 -- Validation loss: 0.005991020705550909 - acc: 0.41382876229343213\n",
      " 44%|████▎     | 87/200 [05:15<06:53,  3.66s/it]INFO:root:Epoch: 87\n",
      "INFO:root:Train loss: 0.006832956802099943 - acc: 0.8380583824768403 -- Validation loss: 0.0077030472457408905 - acc: 0.31411668243799895\n",
      " 44%|████▍     | 88/200 [05:19<06:47,  3.64s/it]INFO:root:Epoch: 88\n",
      "INFO:root:Train loss: 0.006882642395794392 - acc: 0.8362099616284205 -- Validation loss: 0.006093115545809269 - acc: 0.29329519219775235\n",
      " 44%|████▍     | 89/200 [05:22<06:42,  3.63s/it]INFO:root:Epoch: 89\n",
      "INFO:root:Train loss: 0.006868688389658928 - acc: 0.8368241453445447 -- Validation loss: 0.004740328062325716 - acc: 0.5641140495099393\n",
      " 45%|████▌     | 90/200 [05:26<06:41,  3.65s/it]INFO:root:Epoch: 90\n",
      "INFO:root:Train loss: 0.006867858115583658 - acc: 0.8376610288361396 -- Validation loss: 0.0046163639053702354 - acc: 0.5661592628463711\n",
      " 46%|████▌     | 91/200 [05:30<06:39,  3.67s/it]INFO:root:Epoch: 91\n",
      "INFO:root:Train loss: 0.006583386100828648 - acc: 0.8437608367016765 -- Validation loss: 0.006051961332559586 - acc: 0.32647310796852547\n",
      " 46%|████▌     | 92/200 [05:33<06:35,  3.66s/it]INFO:root:Epoch: 92\n",
      "INFO:root:Train loss: 0.006680007092654705 - acc: 0.84117930912975 -- Validation loss: 0.005665135104209185 - acc: 0.4757066629825175\n",
      " 46%|████▋     | 93/200 [05:37<06:29,  3.64s/it]INFO:root:Epoch: 93\n",
      "INFO:root:Train loss: 0.00670688645914197 - acc: 0.8412923308995544 -- Validation loss: 0.005290873348712921 - acc: 0.5072394554002413\n",
      " 47%|████▋     | 94/200 [05:41<06:25,  3.64s/it]INFO:root:Epoch: 94\n",
      "INFO:root:Train loss: 0.006670908536761999 - acc: 0.8419618097594805 -- Validation loss: 0.00695567624643445 - acc: 0.3941319558295754\n",
      " 48%|████▊     | 95/200 [05:44<06:21,  3.63s/it]INFO:root:Epoch: 95\n",
      "INFO:root:Train loss: 0.006595783866941929 - acc: 0.8437656442419906 -- Validation loss: 0.004907174501568079 - acc: 0.5177471089404242\n",
      " 48%|████▊     | 96/200 [05:48<06:17,  3.63s/it]INFO:root:Epoch: 96\n",
      "INFO:root:Train loss: 0.0067968969233334064 - acc: 0.8389308922174472 -- Validation loss: 0.00609446968883276 - acc: 0.4105989807472523\n",
      " 48%|████▊     | 97/200 [05:51<06:13,  3.63s/it]INFO:root:Epoch: 97\n",
      "INFO:root:Train loss: 0.00679133553057909 - acc: 0.8388153987774251 -- Validation loss: 0.007749354001134634 - acc: 0.24524791259014944\n",
      " 49%|████▉     | 98/200 [05:55<06:12,  3.65s/it]INFO:root:Epoch: 98\n",
      "INFO:root:Train loss: 0.006642518565058708 - acc: 0.8420565446667841 -- Validation loss: 0.004977939650416374 - acc: 0.42231021052353146\n",
      " 50%|████▉     | 99/200 [05:59<06:08,  3.65s/it]INFO:root:Epoch: 99\n",
      "INFO:root:Train loss: 0.006589632946997881 - acc: 0.8436445445297417 -- Validation loss: 0.006463754456490278 - acc: 0.3630304394323065\n",
      " 50%|█████     | 100/200 [06:02<06:04,  3.64s/it]INFO:root:Epoch: 100\n",
      "INFO:root:Train loss: 0.006645148154348135 - acc: 0.8420922999081603 -- Validation loss: 0.007156161591410637 - acc: 0.30566187339684037\n",
      " 50%|█████     | 101/200 [06:06<05:59,  3.64s/it]INFO:root:Epoch: 101\n",
      "INFO:root:Train loss: 0.0064839995466172695 - acc: 0.8460987554505426 -- Validation loss: 0.007184599060565233 - acc: 0.35105295668022596\n",
      " 51%|█████     | 102/200 [06:10<05:55,  3.63s/it]INFO:root:Epoch: 102\n",
      "INFO:root:Train loss: 0.006503373384475708 - acc: 0.8463181573896945 -- Validation loss: 0.004962898790836334 - acc: 0.5553806499871807\n",
      " 52%|█████▏    | 103/200 [06:13<05:55,  3.66s/it]INFO:root:Epoch: 103\n",
      "INFO:root:Train loss: 0.0063937027007341385 - acc: 0.8482555675781023 -- Validation loss: 0.006062028929591179 - acc: 0.4331185092472106\n",
      " 52%|█████▏    | 104/200 [06:17<05:49,  3.64s/it]INFO:root:Epoch: 104\n",
      "INFO:root:Train loss: 0.006465436890721321 - acc: 0.8467805800545143 -- Validation loss: 0.006012439262121916 - acc: 0.3690493687602149\n",
      " 52%|█████▎    | 105/200 [06:21<05:46,  3.65s/it]INFO:root:Epoch: 105\n",
      "INFO:root:Train loss: 0.0064663938246667385 - acc: 0.8462339277133851 -- Validation loss: 0.006025802809745073 - acc: 0.4665403053768892\n",
      " 53%|█████▎    | 106/200 [06:24<05:41,  3.63s/it]INFO:root:Epoch: 106\n",
      "INFO:root:Train loss: 0.006265866570174694 - acc: 0.8509981283049723 -- Validation loss: 0.005601773504167795 - acc: 0.43028989217274494\n",
      " 54%|█████▎    | 107/200 [06:28<05:39,  3.65s/it]INFO:root:Epoch: 107\n",
      "INFO:root:Train loss: 0.006338139995932579 - acc: 0.8494098047218034 -- Validation loss: 0.00642814626917243 - acc: 0.11282144935167648\n",
      " 54%|█████▍    | 108/200 [06:32<05:36,  3.66s/it]INFO:root:Epoch: 108\n",
      "INFO:root:Train loss: 0.006149851251393557 - acc: 0.8540559978592088 -- Validation loss: 0.0067490809597074986 - acc: 0.12201201894591551\n",
      " 55%|█████▍    | 109/200 [06:35<05:31,  3.65s/it]INFO:root:Epoch: 109\n",
      "INFO:root:Train loss: 0.006069815717637539 - acc: 0.856413309001811 -- Validation loss: 0.005378102883696556 - acc: 0.4365488417162974\n",
      " 55%|█████▌    | 110/200 [06:39<05:27,  3.64s/it]INFO:root:Epoch: 110\n",
      "INFO:root:Train loss: 0.006174030713737011 - acc: 0.8534331802051828 -- Validation loss: 0.005832708440721035 - acc: 0.44030997886294665\n",
      " 56%|█████▌    | 111/200 [06:43<05:29,  3.70s/it]INFO:root:Epoch: 111\n",
      "INFO:root:Train loss: 0.00607339246198535 - acc: 0.856167437045375 -- Validation loss: 0.005778128281235695 - acc: 0.4656724197941955\n",
      " 56%|█████▌    | 112/200 [06:46<05:28,  3.74s/it]INFO:root:Epoch: 112\n",
      "INFO:root:Train loss: 0.006083297543227673 - acc: 0.8555763985486982 -- Validation loss: 0.007549349218606949 - acc: 0.17539319134929277\n",
      " 56%|█████▋    | 113/200 [06:50<05:23,  3.72s/it]INFO:root:Epoch: 113\n",
      "INFO:root:Train loss: 0.005918978713452816 - acc: 0.8596683269473822 -- Validation loss: 0.004473032895475626 - acc: 0.5143732583254494\n",
      " 57%|█████▋    | 114/200 [06:54<05:17,  3.69s/it]INFO:root:Epoch: 114\n",
      "INFO:root:Train loss: 0.0060828509740531445 - acc: 0.8554939837112359 -- Validation loss: 0.0061974856071174145 - acc: 0.3740644933593035\n",
      " 57%|█████▊    | 115/200 [06:57<05:12,  3.68s/it]INFO:root:Epoch: 115\n",
      "INFO:root:Train loss: 0.006103214342147112 - acc: 0.8553305082237064 -- Validation loss: 0.00598384253680706 - acc: 0.2925858876768922\n",
      " 58%|█████▊    | 116/200 [07:01<05:08,  3.67s/it]INFO:root:Epoch: 116\n",
      "INFO:root:Train loss: 0.006136348471045494 - acc: 0.8542127633014445 -- Validation loss: 0.005495473276823759 - acc: 0.3862107571319249\n",
      " 58%|█████▊    | 117/200 [07:05<05:02,  3.65s/it]INFO:root:Epoch: 117\n",
      "INFO:root:Train loss: 0.0059178974479436874 - acc: 0.8595811511053224 -- Validation loss: 0.004405158571898937 - acc: 0.5851730808286177\n",
      " 59%|█████▉    | 118/200 [07:08<04:57,  3.63s/it]INFO:root:Epoch: 118\n",
      "INFO:root:Train loss: 0.005872893147170544 - acc: 0.8605930910161572 -- Validation loss: 0.005836935248225927 - acc: 0.3609181403380317\n",
      " 60%|█████▉    | 119/200 [07:12<04:55,  3.64s/it]INFO:root:Epoch: 119\n",
      "INFO:root:Train loss: 0.005912514869123697 - acc: 0.8602276860285539 -- Validation loss: 0.006326615344733 - acc: 0.37634117853874516\n",
      " 60%|██████    | 120/200 [07:16<04:52,  3.66s/it]INFO:root:Epoch: 120\n",
      "INFO:root:Train loss: 0.005931255407631397 - acc: 0.859085871252216 -- Validation loss: 0.006493742112070322 - acc: 0.4488887367370493\n",
      " 60%|██████    | 121/200 [07:19<04:47,  3.63s/it]INFO:root:Epoch: 121\n",
      "INFO:root:Train loss: 0.005675042979419231 - acc: 0.8652607075281137 -- Validation loss: 0.006978288758546114 - acc: 0.36186132188432485\n",
      " 61%|██████    | 122/200 [07:23<04:42,  3.62s/it]INFO:root:Epoch: 122\n",
      "INFO:root:Train loss: 0.005744630936533213 - acc: 0.8640890187239796 -- Validation loss: 0.006603544112294912 - acc: -0.10163390629625058\n",
      " 62%|██████▏   | 123/200 [07:26<04:39,  3.62s/it]INFO:root:Epoch: 123\n",
      "INFO:root:Train loss: 0.005653311498463154 - acc: 0.8660202865440543 -- Validation loss: 0.005772855132818222 - acc: 0.43832006337717544\n",
      " 62%|██████▏   | 124/200 [07:30<04:34,  3.61s/it]INFO:root:Epoch: 124\n",
      "INFO:root:Train loss: 0.00581338070333004 - acc: 0.862308011209497 -- Validation loss: 0.005898147355765104 - acc: 0.43714851161413504\n",
      " 62%|██████▎   | 125/200 [07:34<04:29,  3.60s/it]INFO:root:Epoch: 125\n",
      "INFO:root:Train loss: 0.005956273525953293 - acc: 0.8584868514021086 -- Validation loss: 0.005613294430077076 - acc: 0.3202100661064893\n",
      " 63%|██████▎   | 126/200 [07:37<04:27,  3.61s/it]INFO:root:Epoch: 126\n",
      "INFO:root:Train loss: 0.0056673698127269745 - acc: 0.8651264436196611 -- Validation loss: 0.004957739729434252 - acc: 0.5151432492184405\n",
      " 64%|██████▎   | 127/200 [07:41<04:24,  3.62s/it]INFO:root:Epoch: 127\n",
      "INFO:root:Train loss: 0.005736097227782011 - acc: 0.8638130023112219 -- Validation loss: 0.006590031553059816 - acc: 0.3637699817420138\n",
      " 64%|██████▍   | 128/200 [07:44<04:20,  3.62s/it]INFO:root:Epoch: 128\n",
      "INFO:root:Train loss: 0.005593451205641031 - acc: 0.8671015324102247 -- Validation loss: 0.005804345011711121 - acc: 0.4528636609705805\n",
      " 64%|██████▍   | 129/200 [07:48<04:16,  3.62s/it]INFO:root:Epoch: 129\n",
      "INFO:root:Train loss: 0.005588052328675985 - acc: 0.8674712535802892 -- Validation loss: 0.005527249537408352 - acc: 0.4389287627044467\n",
      " 65%|██████▌   | 130/200 [07:52<04:13,  3.62s/it]INFO:root:Epoch: 130\n",
      "INFO:root:Train loss: 0.005632251966744661 - acc: 0.8661977292318612 -- Validation loss: 0.004821372218430042 - acc: 0.5922274507728968\n",
      " 66%|██████▌   | 131/200 [07:55<04:09,  3.62s/it]INFO:root:Epoch: 131\n",
      "INFO:root:Train loss: 0.005436164326965809 - acc: 0.8712863616044307 -- Validation loss: 0.005471542943269014 - acc: 0.40052519691082966\n",
      " 66%|██████▌   | 132/200 [07:59<04:05,  3.61s/it]INFO:root:Epoch: 132\n",
      "INFO:root:Train loss: 0.005542578175663948 - acc: 0.8681176889116935 -- Validation loss: 0.004936132580041885 - acc: 0.6100174988037829\n",
      " 66%|██████▋   | 133/200 [08:03<04:02,  3.63s/it]INFO:root:Epoch: 133\n",
      "INFO:root:Train loss: 0.005541177932173014 - acc: 0.8690767197509355 -- Validation loss: 0.0046991691924631596 - acc: 0.5527961016307509\n",
      " 67%|██████▋   | 134/200 [08:06<03:59,  3.63s/it]INFO:root:Epoch: 134\n",
      "INFO:root:Train loss: 0.00540766678750515 - acc: 0.8720157997060417 -- Validation loss: 0.00666460394859314 - acc: 0.36842411965304345\n",
      " 68%|██████▊   | 135/200 [08:10<03:55,  3.62s/it]INFO:root:Epoch: 135\n",
      "INFO:root:Train loss: 0.005439871456474066 - acc: 0.8707833535353936 -- Validation loss: 0.006892634555697441 - acc: 0.24738597411775598\n",
      " 68%|██████▊   | 136/200 [08:13<03:51,  3.61s/it]INFO:root:Epoch: 136\n",
      "INFO:root:Train loss: 0.005505668930709362 - acc: 0.8690396039882772 -- Validation loss: 0.006006534676998854 - acc: 0.3145684658746495\n",
      " 68%|██████▊   | 137/200 [08:17<03:47,  3.61s/it]INFO:root:Epoch: 137\n",
      "INFO:root:Train loss: 0.005398748442530632 - acc: 0.8718335664051194 -- Validation loss: 0.007395917549729347 - acc: 0.20380239768347674\n",
      " 69%|██████▉   | 138/200 [08:21<03:45,  3.64s/it]INFO:root:Epoch: 138\n",
      "INFO:root:Train loss: 0.0054282816126942635 - acc: 0.871189606241922 -- Validation loss: 0.005924651864916086 - acc: 0.3164173941976871\n",
      " 70%|██████▉   | 139/200 [08:24<03:41,  3.63s/it]INFO:root:Epoch: 139\n",
      "INFO:root:Train loss: 0.005425735376775265 - acc: 0.8713800352501953 -- Validation loss: 0.005013722460716963 - acc: 0.5322834571024314\n",
      " 70%|███████   | 140/200 [08:28<03:37,  3.63s/it]INFO:root:Epoch: 140\n",
      "INFO:root:Train loss: 0.0053907823748886585 - acc: 0.8723413936328933 -- Validation loss: 0.005826830863952637 - acc: 0.3244285311426046\n",
      " 70%|███████   | 141/200 [08:32<03:34,  3.64s/it]INFO:root:Epoch: 141\n",
      "INFO:root:Train loss: 0.005298999138176441 - acc: 0.8739834785981739 -- Validation loss: 0.006646376568824053 - acc: 0.293616405748535\n",
      " 71%|███████   | 142/200 [08:35<03:30,  3.63s/it]INFO:root:Epoch: 142\n",
      "INFO:root:Train loss: 0.005301187746226788 - acc: 0.8741719066831057 -- Validation loss: 0.005729540716856718 - acc: 0.4030513808817008\n",
      " 72%|███████▏  | 143/200 [08:39<03:26,  3.62s/it]INFO:root:Epoch: 143\n",
      "INFO:root:Train loss: 0.005279377102851868 - acc: 0.8749511206989067 -- Validation loss: 0.0048445784486830235 - acc: 0.5255955004989864\n",
      " 72%|███████▏  | 144/200 [08:42<03:22,  3.61s/it]INFO:root:Epoch: 144\n",
      "INFO:root:Train loss: 0.005506135988980532 - acc: 0.8693025337507991 -- Validation loss: 0.00485599972307682 - acc: 0.5641851504526224\n",
      " 72%|███████▎  | 145/200 [08:46<03:18,  3.61s/it]INFO:root:Epoch: 145\n",
      "INFO:root:Train loss: 0.0052990117110311985 - acc: 0.8741404152630533 -- Validation loss: 0.006555774249136448 - acc: 0.2654733461019265\n",
      " 73%|███████▎  | 146/200 [08:50<03:15,  3.61s/it]INFO:root:Epoch: 146\n",
      "INFO:root:Train loss: 0.005327886436134577 - acc: 0.8734695958179957 -- Validation loss: 0.007114201318472624 - acc: 0.3798072501897988\n",
      " 74%|███████▎  | 147/200 [08:53<03:11,  3.62s/it]INFO:root:Epoch: 147\n",
      "INFO:root:Train loss: 0.005270793568342924 - acc: 0.8747742189971311 -- Validation loss: 0.005230600014328957 - acc: 0.4333334463909304\n",
      " 74%|███████▍  | 148/200 [08:57<03:08,  3.63s/it]INFO:root:Epoch: 148\n",
      "INFO:root:Train loss: 0.005339623428881168 - acc: 0.8734731125445607 -- Validation loss: 0.006763855926692486 - acc: 0.282938345389425\n",
      " 74%|███████▍  | 149/200 [09:01<03:04,  3.62s/it]INFO:root:Epoch: 149\n",
      "INFO:root:Train loss: 0.0051451511681079865 - acc: 0.8781507838067585 -- Validation loss: 0.005194967146962881 - acc: 0.4672215481356201\n",
      " 75%|███████▌  | 150/200 [09:04<03:02,  3.64s/it]INFO:root:Epoch: 150\n",
      "INFO:root:Train loss: 0.005256456322968006 - acc: 0.8753686655238175 -- Validation loss: 0.005862358491867781 - acc: 0.25212148614268604\n",
      " 76%|███████▌  | 151/200 [09:08<02:59,  3.66s/it]INFO:root:Epoch: 151\n",
      "INFO:root:Train loss: 0.005143110640347004 - acc: 0.8780768428363297 -- Validation loss: 0.006092674098908901 - acc: 0.3882333992992999\n",
      " 76%|███████▌  | 152/200 [09:12<02:55,  3.65s/it]INFO:root:Epoch: 152\n",
      "INFO:root:Train loss: 0.005145855713635683 - acc: 0.8780900270295611 -- Validation loss: 0.005708928685635328 - acc: 0.3172789106545434\n",
      " 76%|███████▋  | 153/200 [09:15<02:51,  3.66s/it]INFO:root:Epoch: 153\n",
      "INFO:root:Train loss: 0.005188384093344212 - acc: 0.8771198159795074 -- Validation loss: 0.005107059609144926 - acc: 0.5762850511153159\n",
      " 77%|███████▋  | 154/200 [09:19<02:48,  3.65s/it]INFO:root:Epoch: 154\n",
      "INFO:root:Train loss: 0.005220456980168819 - acc: 0.8761463988627868 -- Validation loss: 0.005707847885787487 - acc: 0.41814439637755285\n",
      " 78%|███████▊  | 155/200 [09:23<02:44,  3.65s/it]INFO:root:Epoch: 155\n",
      "INFO:root:Train loss: 0.0052014864049851894 - acc: 0.8766514525667473 -- Validation loss: 0.006214355118572712 - acc: 0.18182336485663708\n",
      " 78%|███████▊  | 156/200 [09:26<02:39,  3.63s/it]INFO:root:Epoch: 156\n",
      "INFO:root:Train loss: 0.004976434633135796 - acc: 0.8823919559694553 -- Validation loss: 0.006296491250395775 - acc: 0.41194631444537455\n",
      " 78%|███████▊  | 157/200 [09:30<02:35,  3.63s/it]INFO:root:Epoch: 157\n",
      "INFO:root:Train loss: 0.005010636523365974 - acc: 0.8810273331347162 -- Validation loss: 0.005567468237131834 - acc: 0.4421977771000821\n",
      " 79%|███████▉  | 158/200 [09:33<02:32,  3.62s/it]INFO:root:Epoch: 158\n",
      "INFO:root:Train loss: 0.0050060744397342205 - acc: 0.8811487690959005 -- Validation loss: 0.005131797399371862 - acc: 0.42278191490636263\n",
      " 80%|███████▉  | 159/200 [09:37<02:27,  3.61s/it]INFO:root:Epoch: 159\n",
      "INFO:root:Train loss: 0.005083192139863968 - acc: 0.8790976739559416 -- Validation loss: 0.006052093114703894 - acc: 0.4377380452091213\n",
      " 80%|████████  | 160/200 [09:41<02:24,  3.62s/it]INFO:root:Epoch: 160\n",
      "INFO:root:Train loss: 0.005181105807423592 - acc: 0.8771251437585266 -- Validation loss: 0.007288639433681965 - acc: 0.2548254886007557\n",
      " 80%|████████  | 161/200 [09:44<02:20,  3.61s/it]INFO:root:Epoch: 161\n",
      "INFO:root:Train loss: 0.00496251555159688 - acc: 0.8821518919067807 -- Validation loss: 0.006046188063919544 - acc: 0.420839336655242\n",
      " 81%|████████  | 162/200 [09:48<02:17,  3.62s/it]INFO:root:Epoch: 162\n",
      "INFO:root:Train loss: 0.004955236800014973 - acc: 0.8824256881352128 -- Validation loss: 0.006711720488965511 - acc: 0.31828713625838645\n",
      " 82%|████████▏ | 163/200 [09:51<02:13,  3.60s/it]INFO:root:Epoch: 163\n",
      "INFO:root:new model saved\n",
      "INFO:root:Train loss: 0.004979533609002829 - acc: 0.8816056577327628 -- Validation loss: 0.004736702889204025 - acc: 0.6378135427258196\n",
      " 82%|████████▏ | 164/200 [09:55<02:11,  3.64s/it]INFO:root:Epoch: 164\n",
      "INFO:root:Train loss: 0.004774533677846193 - acc: 0.8863399295019077 -- Validation loss: 0.00594877265393734 - acc: 0.3780441173230973\n",
      " 82%|████████▎ | 165/200 [09:59<02:07,  3.63s/it]INFO:root:Epoch: 165\n",
      "INFO:root:Train loss: 0.004855428822338581 - acc: 0.885581068375614 -- Validation loss: 0.007393762469291687 - acc: 0.2293512039272535\n",
      " 83%|████████▎ | 166/200 [10:02<02:02,  3.61s/it]INFO:root:Epoch: 166\n",
      "INFO:root:Train loss: 0.004905725363641977 - acc: 0.8835535750613042 -- Validation loss: 0.006550286430865526 - acc: 0.2965867940252179\n",
      " 84%|████████▎ | 167/200 [10:06<01:59,  3.61s/it]INFO:root:Epoch: 167\n",
      "INFO:root:Train loss: 0.00488802557811141 - acc: 0.8841035478762184 -- Validation loss: 0.006490228697657585 - acc: 0.3909972819864348\n",
      " 84%|████████▍ | 168/200 [10:09<01:55,  3.61s/it]INFO:root:Epoch: 168\n",
      "INFO:root:Train loss: 0.005005780141800642 - acc: 0.8816086253866119 -- Validation loss: 0.0055624679662287235 - acc: 0.4977615477500742\n",
      " 84%|████████▍ | 169/200 [10:13<01:51,  3.61s/it]INFO:root:Epoch: 169\n",
      "INFO:root:Train loss: 0.004951602779328823 - acc: 0.8822462240141623 -- Validation loss: 0.007366697769612074 - acc: 0.2324206137986553\n",
      " 85%|████████▌ | 170/200 [10:17<01:48,  3.61s/it]INFO:root:Epoch: 170\n",
      "INFO:root:Train loss: 0.0048406352289021015 - acc: 0.8855740799864973 -- Validation loss: 0.006915166508406401 - acc: 0.36109219000404347\n",
      " 86%|████████▌ | 171/200 [10:20<01:44,  3.61s/it]INFO:root:Epoch: 171\n",
      "INFO:root:Train loss: 0.004873669706285 - acc: 0.8844665408739026 -- Validation loss: 0.007062986493110657 - acc: 0.3469042473545104\n",
      " 86%|████████▌ | 172/200 [10:24<01:40,  3.59s/it]INFO:root:Epoch: 172\n",
      "INFO:root:Train loss: 0.005011101718991995 - acc: 0.8808717806026402 -- Validation loss: 0.0060712783597409725 - acc: 0.32480425032956184\n",
      " 86%|████████▋ | 173/200 [10:27<01:37,  3.60s/it]INFO:root:Epoch: 173\n",
      "INFO:root:Train loss: 0.004775439854711294 - acc: 0.8865034100096348 -- Validation loss: 0.005411406047642231 - acc: 0.5395847408716872\n",
      " 87%|████████▋ | 174/200 [10:31<01:33,  3.60s/it]INFO:root:Epoch: 174\n",
      "INFO:root:Train loss: 0.004795481450855732 - acc: 0.8864648120964012 -- Validation loss: 0.007159012369811535 - acc: 0.18461119916739455\n",
      " 88%|████████▊ | 175/200 [10:35<01:30,  3.60s/it]INFO:root:Epoch: 175\n",
      "INFO:root:Train loss: 0.004829000681638718 - acc: 0.885651809120589 -- Validation loss: 0.004916159901767969 - acc: 0.5784381703554901\n",
      " 88%|████████▊ | 176/200 [10:38<01:26,  3.62s/it]INFO:root:Epoch: 176\n",
      "INFO:root:Train loss: 0.00477720657363534 - acc: 0.8869166309105302 -- Validation loss: 0.005964627955108881 - acc: 0.38048526282141093\n",
      " 88%|████████▊ | 177/200 [10:42<01:23,  3.63s/it]INFO:root:Epoch: 177\n",
      "INFO:root:Train loss: 0.0048393006436526775 - acc: 0.8850424263162742 -- Validation loss: 0.005588277243077755 - acc: 0.41847035176203373\n",
      " 89%|████████▉ | 178/200 [10:46<01:19,  3.61s/it]INFO:root:Epoch: 178\n",
      "INFO:root:Train loss: 0.00494732428342104 - acc: 0.882557270281312 -- Validation loss: 0.007000254467129707 - acc: 0.28897946904999805\n",
      " 90%|████████▉ | 179/200 [10:49<01:15,  3.61s/it]INFO:root:Epoch: 179\n",
      "INFO:root:Train loss: 0.0049142357893288136 - acc: 0.8835659804089842 -- Validation loss: 0.005660645663738251 - acc: 0.36125921006691253\n",
      " 90%|█████████ | 180/200 [10:53<01:13,  3.65s/it]INFO:root:Epoch: 180\n",
      "INFO:root:Train loss: 0.0048449779860675335 - acc: 0.8849802230733518 -- Validation loss: 0.00642356975004077 - acc: 0.3636439687826166\n",
      " 90%|█████████ | 181/200 [10:57<01:09,  3.65s/it]INFO:root:Epoch: 181\n",
      "INFO:root:Train loss: 0.004695804789662361 - acc: 0.8884488560173632 -- Validation loss: 0.005752927623689175 - acc: 0.3808374624350467\n",
      " 91%|█████████ | 182/200 [11:00<01:05,  3.63s/it]INFO:root:Epoch: 182\n",
      "INFO:root:Train loss: 0.004705942701548338 - acc: 0.8884504197928722 -- Validation loss: 0.006657605059444904 - acc: 0.43019566580047086\n",
      " 92%|█████████▏| 183/200 [11:04<01:01,  3.62s/it]INFO:root:Epoch: 183\n",
      "INFO:root:Train loss: 0.004766065627336502 - acc: 0.8873621470837705 -- Validation loss: 0.005984003655612469 - acc: 0.26097427695677744\n",
      " 92%|█████████▏| 184/200 [11:07<00:57,  3.61s/it]INFO:root:Epoch: 184\n",
      "INFO:root:Train loss: 0.004651369992643595 - acc: 0.8894141184330797 -- Validation loss: 0.006744355894625187 - acc: 0.3138438575772122\n",
      " 92%|█████████▎| 185/200 [11:11<00:54,  3.61s/it]INFO:root:Epoch: 185\n",
      "INFO:root:Train loss: 0.004629322327673435 - acc: 0.8899946888590411 -- Validation loss: 0.005115903448313475 - acc: 0.502524683185101\n",
      " 93%|█████████▎| 186/200 [11:15<00:50,  3.60s/it]INFO:root:Epoch: 186\n",
      "INFO:root:Train loss: 0.004743673373013735 - acc: 0.8875424109234239 -- Validation loss: 0.006231945939362049 - acc: 0.515402322507541\n",
      " 94%|█████████▎| 187/200 [11:18<00:46,  3.60s/it]INFO:root:Epoch: 187\n",
      "INFO:root:Train loss: 0.0046415007673203945 - acc: 0.8897545552268082 -- Validation loss: 0.0059538427740335464 - acc: 0.38963266318717094\n",
      " 94%|█████████▍| 188/200 [11:22<00:43,  3.61s/it]INFO:root:Epoch: 188\n",
      "INFO:root:Train loss: 0.00464026490226388 - acc: 0.8900713563163728 -- Validation loss: 0.006172113120555878 - acc: 0.43796773490470453\n",
      " 94%|█████████▍| 189/200 [11:25<00:40,  3.64s/it]INFO:root:Epoch: 189\n",
      "INFO:root:Train loss: 0.004706902429461479 - acc: 0.8881272068126485 -- Validation loss: 0.005794362165033817 - acc: 0.4333780105921977\n",
      " 95%|█████████▌| 190/200 [11:29<00:36,  3.64s/it]INFO:root:Epoch: 190\n",
      "INFO:root:Train loss: 0.0045514460653066635 - acc: 0.8922342756187273 -- Validation loss: 0.005967173259705305 - acc: 0.43179697170061637\n",
      " 96%|█████████▌| 191/200 [11:33<00:32,  3.64s/it]INFO:root:Epoch: 191\n",
      "INFO:root:Train loss: 0.004765247460454702 - acc: 0.8869509907713173 -- Validation loss: 0.006605605129152536 - acc: 0.48036282788084705\n",
      " 96%|█████████▌| 192/200 [11:36<00:29,  3.63s/it]INFO:root:Epoch: 192\n",
      "INFO:root:Train loss: 0.004717053379863501 - acc: 0.8880533125571335 -- Validation loss: 0.0057158516719937325 - acc: 0.5126775677380101\n",
      " 96%|█████████▋| 193/200 [11:40<00:25,  3.65s/it]INFO:root:Epoch: 193\n",
      "INFO:root:Train loss: 0.004624130204319954 - acc: 0.8905203364617548 -- Validation loss: 0.005472094751894474 - acc: 0.24667495808501783\n",
      " 97%|█████████▋| 194/200 [11:44<00:21,  3.65s/it]INFO:root:Epoch: 194\n",
      "INFO:root:Train loss: 0.004542183130979538 - acc: 0.892492901200115 -- Validation loss: 0.006140383891761303 - acc: 0.40135585782417293\n",
      " 98%|█████████▊| 195/200 [11:47<00:18,  3.63s/it]INFO:root:Epoch: 195\n",
      "INFO:root:Train loss: 0.004592439625412226 - acc: 0.8911118280079072 -- Validation loss: 0.005725686438381672 - acc: 0.3673152392837956\n",
      " 98%|█████████▊| 196/200 [11:51<00:14,  3.65s/it]INFO:root:Epoch: 196\n",
      "INFO:root:Train loss: 0.0046621388755738735 - acc: 0.8896891044934341 -- Validation loss: 0.004411723930388689 - acc: 0.5527797719575958\n",
      " 98%|█████████▊| 197/200 [11:55<00:10,  3.66s/it]INFO:root:Epoch: 197\n",
      "INFO:root:Train loss: 0.004644888453185558 - acc: 0.889697789552434 -- Validation loss: 0.004858363885432482 - acc: 0.5741458761406008\n",
      " 99%|█████████▉| 198/200 [11:58<00:07,  3.67s/it]INFO:root:Epoch: 198\n",
      "INFO:root:Train loss: 0.004673819988965988 - acc: 0.8893621840085324 -- Validation loss: 0.006583184935152531 - acc: 0.28948482186648017\n",
      "100%|█████████▉| 199/200 [12:02<00:03,  3.70s/it]INFO:root:Epoch: 199\n",
      "INFO:root:Train loss: 0.004663678817451 - acc: 0.8890097834799091 -- Validation loss: 0.006000082939863205 - acc: 0.3916757456505303\n",
      "100%|██████████| 200/200 [12:06<00:00,  3.63s/it]\n"
     ]
    }
   ],
   "source": [
    "from train import train_model\n",
    "torch.cuda.empty_cache()\n",
    "optimizer = torch.optim.Adam(params=siamese_lstm_attention.parameters(), lr=learning_rate)\n",
    "\n",
    "best_model, train_losses, train_accs, val_losses, val_accs = train_model(\n",
    "    model=siamese_lstm_attention,\n",
    "    optimizer=optimizer,\n",
    "    dataloader=sick_dataloaders,\n",
    "    data=sick_data,\n",
    "    max_epochs=max_epochs,\n",
    "    config_dict={\n",
    "        \"device\": device,\n",
    "        \"model_name\": \"siamese_lstm_attention\",\n",
    "        \"self_attention_config\": self_attention_config,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b30c587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16848/2766329223.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msick_dataloaders\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'validation'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_losses\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'train_loss'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train_losses_tests'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_losses' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "print(len(sick_dataloaders['validation']))\n",
    "plt.plot(train_losses[:], label='train_loss')\n",
    "plt.legend()\n",
    "plt.savefig('train_losses_tests')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(val_losses[:], label='val_loss')\n",
    "plt.legend()\n",
    "plt.savefig('val_losses_test')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(train_accs, label='train_accs')\n",
    "plt.plot(val_accs, label='val_accs')\n",
    "plt.legend()\n",
    "plt.savefig('accuracies_test')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fe9cef",
   "metadata": {},
   "source": [
    "## Part 4. Evaluation and Analysis (2 points)  \n",
    "Implement function evaluate_test_set to calculate the final accuracy of the performance evaluation metric on the test data.  \n",
    "Compare the result with the original AAAI paper. Сomment on effect of penalty loss on model capacity. Did the inclusion of the self-attention block improve the results? If yes, then how? Can you think of additional techniques to improve the results? Briefly answer these questions in the markdown cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "143154f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Evaluating accuracy on test set\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worst score is 0.8275078982114792 and should be 1.0\n",
      "Best score is 4.999999403953552 and should be 3.9000000953674316\n",
      "tensor([ 36,   4, 472,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1])\n",
      "tensor([ 36,   4, 472,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1])\n",
      "final accuracy on test set is 0.35640038968327514\n",
      "pearsons r test set is (0.6973031927735132, 0.0)\n",
      "spearman rho on test set is SpearmanrResult(correlation=0.586403251964487, pvalue=0.0)\n",
      "mean square error on test set is 0.026223165914416313\n",
      "['person', 'practicing', 'snowboarding', 'jumps', 'air', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['person', 'frying', 'meat', 'pan', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['cats', 'playing', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['cats', 'playing', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "similarity: 2.67 for sentences  the cat sits outside <pad> <pad> - the dog plays in the garden\n",
      "similarity: 2.26 for sentences  a man is playing guitar <pad> - a woman watches TV <pad> <pad>\n",
      "similarity: 4.98 for sentences  the new movie is awesome <pad> - the new movie is so great\n",
      "similarity: 5.00 for sentences  the dog plays in the garden - the dog plays in the garden\n",
      "similarity: 3.61 for sentences  i like ice skating <pad> <pad> - this mountain is beautiful <pad> <pad>\n",
      "torch.Size([5, 20, 6])\n",
      "tensor([0.1347, 0.0999, 0.0961, 0.1161, 0.1794, 0.3737], device='cuda:0',\n",
      "       grad_fn=<MeanBackward1>)\n",
      "tensor([0.1380, 0.1024, 0.0980, 0.1170, 0.1780, 0.3665], device='cuda:0',\n",
      "       grad_fn=<MeanBackward1>)\n",
      "tensor([0.1371, 0.1019, 0.0977, 0.1169, 0.1786, 0.3678], device='cuda:0',\n",
      "       grad_fn=<MeanBackward1>)\n",
      "tensor([0.1441, 0.1061, 0.0998, 0.1172, 0.1754, 0.3574], device='cuda:0',\n",
      "       grad_fn=<MeanBackward1>)\n",
      "tensor([0.1370, 0.1017, 0.0972, 0.1161, 0.1779, 0.3701], device='cuda:0',\n",
      "       grad_fn=<MeanBackward1>)\n"
     ]
    }
   ],
   "source": [
    "from test import evaluate_test_set\n",
    "load_model=True\n",
    "if load_model:\n",
    "    best_model = SiameseBiLSTMAttention(\n",
    "        batch_size=batch_size,\n",
    "        output_size=output_size,\n",
    "        hidden_size=hidden_size,\n",
    "        vocab_size=vocab_size,\n",
    "        embedding_size=embedding_size,\n",
    "        embedding_weights=embedding_weights,\n",
    "        lstm_layers=lstm_layers,\n",
    "        self_attention_config=self_attention_config,\n",
    "        attention_encoder_config=attention_encoder_config,\n",
    "        fc_hidden_size=fc_hidden_size,\n",
    "        device=device,\n",
    "        pad_index=pad_index,\n",
    "        bidirectional=bidirectional,\n",
    "        dropout=dropout\n",
    "    )\n",
    "    # move model to device\n",
    "    best_model.to(device)\n",
    "    best_model.load_state_dict(torch.load(\"siamese_lstm_attention.pth\"))\n",
    "    best_model.eval()\n",
    "\n",
    "\n",
    "test_acc, r, rho,mse, least_1, least_2, most_1, most_2  = evaluate_test_set(\n",
    "    model=best_model,\n",
    "    data_loader=sick_dataloaders,\n",
    "    config_dict={\n",
    "        \"device\": device,\n",
    "        \"model_name\": \"siamese_lstm_attention\",\n",
    "        \"self_attention_config\": self_attention_config,\n",
    "    },\n",
    ")\n",
    "\n",
    "print('final accuracy on test set is {}'.format(test_acc))\n",
    "print('pearsons r test set is {}'.format(r))\n",
    "print('spearman rho on test set is {}'.format(rho))\n",
    "print('mean square error on test set is {}'.format(mse))\n",
    "\n",
    "worst_sen_1 = [sick_data.vocab.itos[token] for token in least_1]\n",
    "print(worst_sen_1)\n",
    "worst_sen_2 = [sick_data.vocab.itos[token] for token in least_2]\n",
    "print(worst_sen_2)\n",
    "\n",
    "best_sen_1 = [sick_data.vocab.itos[token] for token in most_1]\n",
    "print(best_sen_1)\n",
    "best_sen_2 = [sick_data.vocab.itos[token] for token in most_2]\n",
    "print(best_sen_2)\n",
    "\n",
    "\n",
    "\n",
    "## Test on some manual sentences\n",
    "test_sentences1 = [ 'the cat sits outside <pad> <pad>',\n",
    "                    'a man is playing guitar <pad>',\n",
    "                    'the new movie is awesome <pad>',\n",
    "                    'the dog plays in the garden',\n",
    "                    'i like ice skating <pad> <pad>']\n",
    "\n",
    "test_sentences2 = [ 'the dog plays in the garden',\n",
    "                    'a woman watches TV <pad> <pad>',\n",
    "                    'the new movie is so great',\n",
    "                    'the dog plays in the garden',\n",
    "                    'this mountain is beautiful <pad> <pad>']\n",
    "\n",
    "tokenized_sen1= list()             \n",
    "for sentence in test_sentences1:\n",
    "    tokenized = [sick_data.vocab.stoi[word] for word in sentence.split()]\n",
    "    tokenized_sen1.append(tokenized)\n",
    "tokenized_sen1 = torch.tensor(tokenized_sen1).to(device)\n",
    "\n",
    "tokenized_sen2= list()             \n",
    "for sentence in test_sentences2:\n",
    "    tokenized = [sick_data.vocab.stoi[word] for word in sentence.split()]\n",
    "    tokenized_sen2.append(tokenized)\n",
    "tokenized_sen2=torch.tensor(tokenized_sen2).to(device)\n",
    "\n",
    "\n",
    "predic, a1, a2 = best_model(tokenized_sen1, tokenized_sen2)\n",
    "\n",
    "for i in range(len(test_sentences1)):\n",
    "    print('similarity: {:.2f} for sentences  {} - {}'.format(predic[i]*5.0, test_sentences1[i], test_sentences2[i]))\n",
    "\n",
    "#a1 = a1.permute(0,2,1) #(batch, seq_len, scores)\n",
    "#print(a1[2])\n",
    "print(a1.shape)\n",
    "for i in range(len(test_sentences1)):\n",
    "    sen = a1[i]\n",
    "    mean_att = torch.mean(sen, dim=0)\n",
    "    print(torch.sum(mean_att, dim=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9195cd95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
